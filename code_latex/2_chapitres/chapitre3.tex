\chapter{Protocole d'Analyse : Sélection d'une Méthode d'Agrégation}

Après avoir établi le cadre réglementaire et construit un générateur de passif capable de produire des données réalistes, il convient désormais d'aborder la problématique centrale : la réduction de la dimension du portefeuille de passif. L'impossibilité technique de projeter individuellement chaque contrat au sein de modèles stochastiques gourmands en ressources fait de l'agrégation du passif un impératif pour l'ensemble du marché de l'assurance. Cette nécessité découle directement de l'exigence de réactivité lors des phases de clôture, où la finesse de l'analyse doit impérativement se concilier avec une performance des modèles de projection utilisés car il faut effectuer beaucoup de lancements différents.

L'objectif de ce chapitre est de définir et de mettre en œuvre un protocole d'analyse visant à sélectionner la méthode d'agrégation la plus performante pour cette étude. L'enjeu est de résoudre un problème d'optimisation sous contraintes : comment réduire drastiquement la volumétrie des données en entrée du modèle ALM, et par extension les temps de traitement, sans altérer la fidélité des indicateurs de risque prudentiels, à savoir le Best Estimate (BE) et le Solvency Capital Requirement (SCR) ?

Pour répondre à cette question, plusieurs approches d'agrégation seront comparées. Deux grandes familles de méthodes seront étudiées :
\begin{itemize}
    \item \textbf{Les approches déterministes classiques :} Il s'agit des méthodes traditionnellement utilisées sur le marché, reposant sur un regroupement des contrats par caractéristiques communes ou par tranches. Elles serviront de point de référence pour l'étude.
    \item \textbf{Les approches statistiques par apprentissage non supervisé :} L'apport des algorithmes de Clustering, tels que les K-Means ou les méthodes basées sur la densité, sera exploré pour constituer des Model Points basés sur une similarité multidimensionnelle des risques.
\end{itemize}

La démarche s'articulera en trois temps. Les spécificités mathématiques et algorithmiques de chaque méthode candidate seront d'abord exposées. Il s'agira également de détailler le fonctionnement des algorithmes d'apprentissage non supervisé, dont l'usage reste encore peu répandu dans les pratiques actuarielles. Un protocole de test standardisé sera ensuite défini, en précisant les résultats sur le portefeuille de référence et les métriques d'évaluation retenues (écart sur le Best Estimate et taux de compression). Enfin, l'analyse comparative des résultats sera réalisée, ce qui permettra de sélectionner la méthode optimale qui sera utilisée pour les analyses de sensibilité du chapitre suivant.

\section{Présentation des Méthodes d'Agrégation candidates}
    \subsection{Approche déterministe par regroupement de caractéristiques}
    
    La méthode par regroupement de caractéristiques repose sur une segmentation par classes de risque, cette approche constitue le standard historique en actuariat. Elle servira de base de comparaison pour évaluer la performance des algorithmes de regroupement statistique plus avancés présentés dans les parties suivantes.

    Le principe fondamental repose sur une partition déterministe du portefeuille. L'objectif est de regrouper les contrats partageant des caractéristiques de risque identiques. Dans un modèle de projection de gestion actif-passif, l'évolution de la Provision Mathématique (PM) est influencée par des caractéristiques spécifiques du contrat. La mortalité dépend de la génération, de l'âge et du sexe ; les rachats sont corrélés à l'âge de l'assuré et à l'ancienneté du contrat (fiscalité) ; la revalorisation dépend du Taux Minimum Garanti (TMG).

    Par conséquent, pour garantir que le Model Point agrégé se comporte comme la somme des contrats individuels, il est impératif de regrouper les polices selon ces axes discriminants. Contrairement aux méthodes statistiques qui cherchent des similarités globales, cette approche applique une segmentation rigide. Les critères de regroupement retenus pour cette étude sont :
    \begin{itemize}
        \item \textbf{Le Sexe} : Indispensable pour l'application des tables de mortalité différenciées.
        \item \textbf{Le Taux Minimum Garanti (TMG)} : Crucial pour la valorisation des garanties financières.
        \item \textbf{L'Âge de l'assuré} : Discrétisé à l'entier inférieur pour correspondre aux entrées des tables de mortalité.
        \item \textbf{L'Ancienneté du contrat} : Discrétisée à l'entier inférieur pour modéliser correctement la fiscalité et les rachats structurels.
    \end{itemize}

    Mathématiquement, cette méthode définit une relation d'équivalence stricte. Deux contrats $i$ et $j$ appartiennent au même Model Point $k$ si et seulement si leurs vecteurs de caractéristiques discrétisées sont identiques :
    $$ (\text{Sexe}_i, \lfloor \text{Age}_i \rfloor, \lfloor \text{Anc}_i \rfloor, \text{TMG}_i) = (\text{Sexe}_j, \lfloor \text{Age}_j \rfloor, \lfloor \text{Anc}_j \rfloor, \text{TMG}_j) $$

    Une fois les groupes constitués, l'agrégation s'opère par la somme des variables extensives (dont la valeur pour le groupe est la somme des valeurs individuelles). La Provision Mathématique du Model Point $k$ est alors la somme des PM des contrats qui le constituent :
    $$ PM_{MP_k} = \sum_{i \in Groupe_k} PM_i $$
    Les variables intensives du Model Point (dont la valeur est indépendante de la taille du groupe, comme l'Âge ou l'Ancienneté) prennent alors les valeurs définies par la segmentation (par exemple, 45 ans et 10 ans), et non une moyenne pondérée, ce qui est cohérent avec la logique de discrétisation à l'entier.

    Cette approche présente l'avantage majeur de la simplicité et de la transparence. Elle conserve exactement la volumétrie financière du portefeuille et respecte scrupuleusement les garanties contractuelles (absence de dilution du TMG par moyenne). Elle garantit une homogénéité parfaite des assurés au sein d'un groupe.

    Cependant, elle présente des limites intrinsèques :
    \begin{itemize} 
        \item \textbf{La rigidité de la structure de regroupement :} La définition des classes est fixée \textit{a priori} et de manière uniforme, sans tenir compte de la distribution réelle des capitaux. Cette approche ne permet pas de concentrer automatiquement la précision du regroupement sur les zones à fort enjeux financiers, traitant potentiellement avec la même granularité les segments marginaux et les segments prépondérants.
        \item \textbf{L'absence de contrôle sur la compression :} Le nombre de Model Points finaux n'est pas paramétrable. Il dépend exclusivement de la dispersion du portefeuille et de la finesse de la segmentation. Sur un portefeuille très hétérogène, cette méthode peut générer un nombre de groupes très élevé, dont certains ne contiendront que peu de contrats, limitant ainsi l'efficacité de la réduction de dimension (phénomène du « fléau de la dimension »).
    \end{itemize}

    \subsection{Approche par tranches et moyenne pondérée}

    Cette méthode constitue une évolution de la segmentation par classes présentée précédemment. Elle vise à réduire davantage le nombre de Model Points en relâchant la contrainte d'égalité stricte sur les variables continues (âge et ancienneté) au profit d'une logique d'intervalles, ou discrétisation par paliers.

    Le principe de regroupement reste déterministe. Les variables catégorielles ou contractuelles majeures (Sexe, TMG) conservent une discrimination stricte. En revanche, l'espace des variables temporelles est découpé en tranches. Dans le cadre de cette implémentation, les intervalles suivant ont été retenus :
    \begin{itemize}
        \item Un pas de 2 ans pour l'âge de l'assuré ;
        \item Un pas de 5 ans pour l'ancienneté du contrat.
    \end{itemize}

    Un contrat $i$ appartient à un groupe $k$ si ses caractéristiques discrètes correspondent et si ses variables continues tombent dans les intervalles définis :
    $$ i \in MP_k \iff \begin{cases} \text{Sexe}_i = \text{Sexe}_k \\ \text{TMG}_i = \text{TMG}_k \\ \text{Age}_i \in [\text{Age}_{min}^k, \text{Age}_{max}^k[ \\ \text{Anciennet\'{e}}_i \in [\text{Anc}_{min}^k, \text{Anc}_{max}^k[ \end{cases} $$

    La spécificité majeure de cette approche réside dans la détermination des caractéristiques du Model Point. Contrairement à une approche simpliste qui retiendrait le centre de l'intervalle (par exemple, 31 ans pour la tranche [30-32[), dans cette méthode il faut calculer le barycentre des contrats regroupés. Finalement, afin de préserver la structure financière du portefeuille, la moyenne est pondérée par la Provision Mathématique (PM).

    Ainsi, l'âge ($Age_{MP}$) et l'ancienneté ($Anc_{MP}$) du Model Point sont calculés comme suit :
    $$ \text{Age}_{MP} = \frac{\sum_{i \in MP} \text{Age}_i \times \text{PM}_i}{\sum_{i \in MP} \text{PM}_i} \quad ; \quad \text{Anc}_{MP} = \frac{\sum_{i \in MP} \text{Anciennet\'{e}}_i \times \text{PM}_i}{\sum_{i \in MP} \text{PM}_i} $$

    Cette pondération par les encours permet de s'assurer que le Model Point est représentatif des contrats les plus significatifs financièrement au sein de la tranche, minimisant ainsi le biais d'agrégation sur les projections de flux futurs.

    \subsection{Approche statistique par apprentissage non supervisé (K-Means)}

    Contrairement aux méthodes déterministes qui segmentent l'espace des risques selon une grille préétablie, les approches par apprentissage non supervisé (Machine Learning) visent à découvrir la structure intrinsèque des données. L'objectif n'est plus d'imposer un regroupement, mais de laisser l'algorithme identifier les zones de forte densité du portefeuille pour y placer les Model Points.
    
    Parmi les algorithmes de clustering, la méthode des K-Means (ou K-Moyennes) a été retenue pour sa robustesse et sa capacité à minimiser l'inertie intra-classe, c'est-à-dire la dispersion des contrats autour de leur Model Point représentatif.

    \subsubsection{Principe de l'algorithme}
    L'algorithme des K-Means cherche à partitionner un ensemble de $N$ contrats en $K$ groupes (ou clusters) distincts, de manière à minimiser la distance entre chaque contrat et le centre de son groupe (le centroïde). Dans notre contexte, ce centroïde deviendra le Model Point.
    
    Mathématiquement, pour un ensemble de contrats représentés par des vecteurs de caractéristiques $x_i \in \mathbb{R}^d$ (où $d$ est le nombre de dimensions : Âge, Ancienneté, TMG...), l'algorithme cherche à déterminer les $K$ centroïdes $\mu_1, ..., \mu_K$ qui minimisent la fonction objectif $J$ (l'inertie) :
    
    $$ J = \sum_{j=1}^{K} \sum_{x_i \in S_j} w_i || x_i - \mu_j ||^2 $$
    
    Où :
    \begin{itemize}
        \item $S_j$ est l'ensemble des contrats assignés au cluster $j$.
        \item $|| . ||$ est la norme euclidienne (distance).
        \item $w_i$ est le poids du contrat $i$. Cette pondération est fondamentale en actuariat : elle correspond à la Provision Mathématique ($PM_i$). Ainsi, l'algorithme est forcé de minimiser l'erreur de regroupement prioritairement pour les contrats à fort enjeux financiers.
    \end{itemize}

    \subsubsection{Application aux données du portefeuille}
    La mise en œuvre de cet algorithme sur le portefeuille d'épargne nécessite plusieurs étapes de traitement des données pour garantir la pertinence des regroupements.

    \paragraph{Choix des variables et Standardisation}
    Les variables retenues pour le calcul de la distance sont celles qui impactent directement le profil de risque et les flux futurs :
    \begin{itemize}
        \item \textbf{L'Âge de l'assuré} (impact mortalité) ;
        \item \textbf{L'Ancienneté fiscale} (impact rachats) ;
        \item \textbf{Le Taux Minimum Garanti (TMG)} (impact revalorisation).
    \end{itemize}
    
    Ces variables ayant des échelles très différentes (un âge varie de 0 à 100, un TMG de 0 à 0.04), un calcul de distance brut donnerait un poids disproportionné à l'âge. Il est donc impératif de procéder à une standardisation (centrage-réduction) des données avant le clustering :
    $$ \tilde{x}_{i, d} = \frac{x_{i, d} - \mu_d}{\sigma_d} $$
    Cette transformation place toutes les variables sur une échelle comparable, permettant à l'algorithme de traiter équitablement les différentes dimensions du risque.

    \paragraph{Algorithme et Pondération}
    L'algorithme utilisé est une variante pondérée des K-Means. Contrairement à une approche standard où chaque point a une importance égale, ici chaque contrat « attire » le centroïde proportionnellement à sa PM.
    Le processus itératif est le suivant :
    \begin{enumerate}
        \item \textbf{Initialisation :} Sélection de $K$ centroïdes initiaux (méthode k-means++ pour optimiser la convergence).
        \item \textbf{Affectation :} Chaque contrat est assigné au centroïde le plus proche en termes de distance euclidienne pondérée.
        \item \textbf{Mise à jour :} Les nouveaux centroïdes sont recalculés comme le barycentre pondéré des contrats de leur cluster.
        \item \textbf{Convergence :} Répétition des étapes 2 et 3 jusqu'à stabilisation des centroïdes.
    \end{enumerate}

    \subsubsection{Constitution des Model Points finaux}
    Une fois la convergence atteinte, chaque cluster $j$ est transformé en un Model Point unique.
    \begin{itemize}
        \item Les variables extensives (PM, Nombre de contrats) sont sommées : $PM_{MP_j} = \sum_{i \in S_j} PM_i$.
        \item Les variables intensives (Âge, Ancienneté, TMG) sont définies par les coordonnées du centroïde final, qui correspondent naturellement à la moyenne pondérée des caractéristiques des contrats du cluster.
    \end{itemize}
    
    Cette méthode permet de définir automatiquement des Model Points situés au cœur des masses financières du portefeuille, comme illustré dans la Figure \ref{fig:kmeans_clusters}, offrant ainsi une représentation optimale de la distribution des risques.

    % \begin{figure}[h!]
    %     \centering
    %     \includegraphics[width=0.8\textwidth]{images/2_chapitres/chapitre3/kmeans_clusters_sexe_H.png}
    %     \caption{Visualisation des clusters K-Means et des Model Points résultants (Projection sur le plan Age/Ancienneté).}
    %     \label{fig:kmeans_clusters}
    % \end{figure}

    \subsection{Approches autres (mémoire Amine, etc.)}
\section{Définition du Protocole de Test Comparatif}
    \subsection{Constitution des portefeuilles de test}
    % Votre texte ici...
    \subsection{Définition des critères de sélection : fidélité des indicateurs (BE/SCR), performance et temps de calcul}
    % Votre texte ici...

\section{Analyse Comparative et Choix de la Méthode Optimale}
    \subsection{Synthèse des performances pour chaque méthode candidate}
    % Votre texte ici...
    \subsection{Justification du choix de la méthode retenue pour l'analyse de sensibilité}
    % Votre texte ici...
% \chapter{Protocole d'Analyse : Agrégation et Scénarios de Sensibilité}

% % Introduction du chapitre : Expliquer que ce chapitre pose toute la méthodologie
% % qui sera appliquée dans la partie suivante. C'est le "comment" de l'analyse.

% \section{Méthodologie et Sélection du Modèle d'Agrégation}
% % Objectif : Décrire le processus complet qui mène au choix d'UNE méthode d'agrégation.

% \subsection{Description des Méthodes d'Agrégation Candidates}
% % Reprendre votre description technique des méthodes (K-means, DBSCAN, etc.)

% \subsection{Tests de Performance et Analyse comparative}
% % Fusionner la présentation des portefeuilles de test et l'analyse
% \subsubsection{Présentation des portefeuilles de test}
% % Décrire les portefeuilles utilisés spécifiquement pour comparer les méthodes d'agrégation.

% \subsubsection{Critères d'évaluation et résultats des tests}
% % Analyser les résultats (BE, SCR, temps de calcul) pour chaque méthode.
% % Inclure ici la réflexion sur l'optimisation du nombre de Model Points.

% \subsection{Choix du Modèle d'Agrégation Optimal pour les métriques S2}
% % Conclure cette section en justifiant le choix d'UNE méthode spécifique
% % au regard des résultats précédents et de sa compatibilité avec les architectures modernes.

% \section{Définition des Scénarios de Sensibilité}
% % Objectif : Décrire précisément les tests qui seront menés sur les portefeuilles.

% \subsection{Création des Portefeuilles de Test via le Générateur}
% % Expliquer comment le générateur est utilisé pour créer les portefeuilles de base
% % ainsi que leurs variations pour les tests de sensibilité.

% \subsection{Description des Chocs et Modifications Appliqués}
% % Détailler les chocs (positifs/négatifs) sur les variables clés.
% % Décrire le scénario d'ajout d'un nouveau produit (caractéristiques, volume, etc.).
