\chapter{Présentation des méthodes d'agrégation candidates}

Après avoir établi le cadre réglementaire et construit un générateur de passif capable de produire des données réalistes, il convient désormais d'aborder la problématique centrale de ce mémoire : la réduction de la dimension du portefeuille de passif. 

La projection individuelle de chaque contrat au sein des modèles ALM requiert des ressources computationnelles excessives ; l'agrégation du passif s'impose donc comme une nécessité technique pour les assureurs. Cette nécessité découle directement des exigences de réactivité lors des phases de clôture réglementaire Solvabilité 2, où la finesse de l'analyse des risques doit impérativement se concilier avec la performance d'exécution pour permettre le lancement de multiples sensibilités et scénarios économiques.

L'enjeu sous-jacent est de résoudre un problème complexe d'optimisation sous contraintes : comment réduire drastiquement la volumétrie des données en entrée du modèle ALM sans altérer la fidélité des indicateurs de risque prudentiels, à savoir le \textit{Best Estimate} (BE) et le \textit{Solvency Capital Requirement} (SCR) ?

Pour répondre à cette problématique, l'objectif de ce chapitre est de définir et d'exposer de manière exhaustive les fondements mathématiques et algorithmiques de différentes approches d'agrégation qui seront mises en concurrence dans ce mémoire. La démarche s'articulera autour de l'étude de trois grandes familles de méthodes :

\begin{itemize}
    \item \textbf{Les approches déterministes classiques :} Méthodes traditionnellement utilisées sur le marché, elles reposent sur une segmentation experte du portefeuille par caractéristiques communes ou par tranches (\textit{Banding}). Elles serviront de référence (ou \textit{benchmark}) pour évaluer l'apport des techniques plus complexes.
    \item \textbf{Les approches statistiques par apprentissage non supervisé :} L'apport des algorithmes de \textit{Machine Learning}, tels que les algorithmes de partitionnement (K-Means), basés sur la densité (DBSCAN, HDBSCAN) ou les arbres de décision (CART), sera exploré pour constituer des \textit{Model Points} fondés sur une similarité multidimensionnelle des profils de risque.
    \item \textbf{Les approches dynamiques basées sur les flux financiers :} En rupture avec le regroupement spatial statique, ces méthodes avancées (telles que le \textit{Cash-Flow Matching} et l'approche par calibration \textit{a posteriori}) proposent d'agréger les contrats en fonction de leur comportement projeté et de leur cinétique d'écoulement.
\end{itemize}

L'exposé détaillé de ces modèles nous permettra de poser le socle théorique nécessaire avant d'aborder, dans le chapitre suivant, le protocole de test standardisé et l'analyse comparative de leurs performances respectives.

\section{Les approches déterministes : Le standard de marché}
\subsection{Segmentation stricte par regroupement de caractéristiques}

La méthode par regroupement de caractéristiques repose sur une segmentation par classes de risque, cette approche constitue le standard historique en actuariat. Elle servira de ba
se de comparaison pour évaluer la performance des algorithmes de regroupement statistique plus avancés présentés dans les parties suivantes.

Le principe fondamental repose sur une partition déterministe du portefeuille. L'objectif est de regrouper les contrats partageant des caractéristiques de risque identiques. Dans 
un modèle de projection de gestion actif-passif, l'évolution de la Provision Mathématique (PM) est influencée par des caractéristiques spécifiques du contrat. La mortalité dépend 
de la génération, de l'âge et du sexe ; les rachats sont corrélés à l'âge de l'assuré et à l'ancienneté du contrat (fiscalité) ; la revalorisation dépend du Taux Minimum Garanti (
TMG).

Par conséquent, pour garantir que le Model Point agrégé se comporte comme la somme des contrats individuels, il est impératif de regrouper les polices selon ces axes discriminants
. Contrairement aux méthodes statistiques qui cherchent des similarités globales, cette approche applique une segmentation rigide. Les critères de regroupement retenus pour cette 
étude sont :
\begin{itemize}
    \item \textbf{Le Sexe} : Indispensable pour l'application des tables de mortalité différenciées.
    \item \textbf{Le Taux Minimum Garanti (TMG)} : Crucial pour la valorisation des garanties financières.
    \item \textbf{L'Âge de l'assuré} : Discrétisé à l'entier inférieur pour correspondre aux entrées des tables de mortalité.
    \item \textbf{L'Ancienneté du contrat} : Discrétisée à l'entier inférieur pour modéliser correctement la fiscalité et les rachats structurels.
\end{itemize}

Mathématiquement, cette méthode définit une relation d'équivalence stricte. Deux contrats $i$ et $j$ appartiennent au même Model Point $k$ si et seulement si leurs vecteurs de car
actéristiques discrétisées sont identiques :
\begin{equation}
(S_i, \lfloor A_i \rfloor, \lfloor Anc_i \rfloor, TMG_i) = (S_j, \lfloor A_j \rfloor, \lfloor Anc_j \rfloor, TMG_j)
\end{equation}

Où $S_i, A_i$ et $Anc_i$ désignent respectivement le sexe, l'âge et l'ancienneté du contrat $i$. Une fois les groupes constitués, l'agrégation s'opère par la somme des variables extensives. La Provision Mathématique du Model Point $k$ (associé à la classe d'équivalence $C_k$) est alors la somme des encours des contrats qui le constituent :
\begin{equation}
PM_{MP_k} = \sum_{i \in C_k} PM_i
\end{equation}
Les variables intensives du Model Point (dont la valeur est indépendante de la taille du groupe, comme l'Âge ou l'Ancienneté) prennent alors les valeurs définies par la segmentation (par exemple, 45 ans et 10 ans), et non une moyenne pondérée, ce qui est cohérent avec la logique de discrétisation à l'entier.

Cette approche présente l'avantage majeur de la simplicité et de la transparence. Elle conserve exactement la volumétrie financière du portefeuille et respecte scrupuleusement les garanties contractuelles (absence de dilution du TMG par moyenne). Elle garantit une homogénéité parfaite des assurés au sein d'un groupe.

Cependant, elle présente des limites intrinsèques :
\begin{itemize}
    \item \textbf{La rigidité de la structure de regroupement :} La définition des classes est fixée \textit{a priori} et de manière uniforme, sans tenir compte de la distribution réelle des capitaux. Cette approche ne permet pas de concentrer automatiquement la précision du regroupement sur les zones à fort enjeux financiers, traitant potentiellement avec la même granularité les segments marginaux et les segments prépondérants.
    \item \textbf{L'absence de contrôle sur la compression :} Le nombre de Model Points finaux n'est pas paramétrable. Il dépend exclusivement de la dispersion du portefeuille et de la finesse de la segmentation. Sur un portefeuille très hétérogène, cette méthode peut générer un nombre de groupes très élevé, dont certains ne contiendront que peu de contrats, limitant ainsi l'efficacité de la réduction de dimension (phénomène du « fléau de la dimension »).
\end{itemize}

\subsection{Approche par tranches (Banding) et pondération financière}

Cette méthode constitue une évolution de la segmentation par classes présentée précédemment. Elle vise à réduire davantage le nombre de Model Points en relâchant la contrainte d'égalité stricte sur les variables continues (âge et ancienneté) au profit d'une logique d'intervalles, ou discrétisation par paliers.

Le principe de regroupement reste déterministe. Les variables catégorielles ou contractuelles majeures (Sexe, TMG) conservent une discrimination stricte. En revanche, l'espace des variables temporelles est découpé en tranches. Dans le cadre de cette implémentation, les intervalles suivant ont été retenus :
\begin{itemize}
    \item Un pas de 2 ans pour l'âge de l'assuré ;
    \item Un pas de 5 ans pour l'ancienneté du contrat.
\end{itemize}

Un contrat $i$ appartient à un groupe $k$ si ses caractéristiques discrètes correspondent et si ses variables continues tombent dans les intervalles définis :

\begin{equation}
i \in C_k \iff 
\begin{cases} 
S_i = S_k \\ 
TMG_i = TMG_k \\ 
A_i \in [A_{min}^k, A_{max}^k[ \\ 
Anc_i \in [Anc_{min}^k, Anc_{max}^k[ 
\end{cases}
\end{equation}

La spécificité majeure de cette approche réside dans la détermination des caractéristiques du Model Point. Contrairement à une approche simpliste qui retiendrait le centre de l'intervalle, il convient de calculer le barycentre financier des contrats regroupés. Ainsi, l'âge ($A_{MP_k}$) et l'ancienneté ($Anc_{MP_k}$) du Model Point $k$ sont calculés comme suit :

\begin{equation}
A_{MP_k} = \frac{\sum_{i \in C_k} A_i \cdot PM_i}{\sum_{i \in C_k} PM_i} \quad ; \quad Anc_{MP_k} = \frac{\sum_{i \in C_k} Anc_i \cdot PM_i}{\sum_{i \in C_k} PM_i}
\end{equation}

Cette pondération par les encours permet de s'assurer que le Model Point est représentatif des contrats les plus significatifs financièrement au sein de la tranche, minimisant ainsi le biais d'agrégation sur les projections de flux futurs.

\subsubsection{Variante Smart Granular et limites (Biais d'agrégation et variance intra-classe)}
Dans le cadre de l'optimisation, une variante nommée \textbf{Smart Granular} a été développée. Contrairement au Banding classique à pas constant, cette approche adapte la finesse des mailles selon la sensibilité locale des risques. Plusieurs profils de découpage ont été implémentés pour répondre à des objectifs d'analyse différents :
\begin{itemize}
    \item \textbf{Smart Focus Active :} Priorise la précision sur la phase de constitution de l'épargne (20-65 ans) avec un pas de 2 ans, tout en compressant fortement les extrêmes (pas de 15 ans au-delà de 85 ans).
    \item \textbf{Smart Focus Young :} Stratégie ultra-fine sur les nouveaux entrants (pas de 1 an jusqu'à 50 ans) pour capturer la dynamique de croissance du portefeuille.       
    \item \textbf{Smart Focus Fiscal :} Découpage spécifique autour de l'ancienneté 8 ans (pas de 1 an entre 0 et 12 ans) pour modéliser avec précision le basculement de la fiscalité des rachats.
    \item \textbf{Smart Granular (Équilibré) :} Mélange des approches précédentes, resserrant les mailles à la fois sur l'âge et sur le cap fiscal des 8 ans.
\end{itemize}
\subsubsection{Limites de l'approche et variance intra-classe}
Bien que la pondération par la Provision Mathématique assure la conservation du barycentre financier, le regroupement par tranches (Banding) écrase inévitablement la dispersion au sein de chaque groupe. Mathématiquement, la variance intra-classe (ou inertie) d'une tranche $k$ pour une caractéristique $X$ (comme l'âge) s'exprime par :

\begin{equation}
 Var(X_k) = \frac{1}{\sum_{i \in MP_k} PM_i} \sum_{i \in MP_k} PM_i (X_i - X_{MP_k})^2 
\end{equation}
Plus la tranche est large, plus cette variance augmente, ce qui dilue la précision du profil de risque. Le modèle ALM traitant les flux de manière non linéaire (notamment via les probabilités de rachat ou de mortalité), cette perte de dispersion engendre le fameux « biais d'agrégation ». C'est cette limitation fondamentale qui justifie le recours à des algorithmes de partitionnement dynamique. Plutôt que de subir une grille d'agrégation statique et arbitraire qui contraint la donnée, il apparaît nécessaire d'adopter des approches où la structure intrinsèque du portefeuille dicte elle-même la formation des groupes. Les méthodes d'apprentissage non supervisé, explorées dans la section suivante, répondent précisément à cet impératif en cherchant à minimiser l'inertie intra-classe de manière algorithmique.


\section{Les approches statistiques spatiales (Apprentissage Non Supervisé)}

\subsection{Préparation de l'espace spatial : Techniques transverses}
Au-delà du choix de l'algorithme de clustering, la performance de l'agrégation dépend fortement de la préparation des données et des contraintes métier imposées lors du processus.

\subsubsection*{Stratification par Risques Majeurs (TMG et Sexe)}
Afin d'éviter des compensations de risques biologiquement ou financièrement absurdes, une stratégie de \textbf{pré-split} a été systématiquement appliquée. Le portefeuille est d'abord divisé en strates étanches selon :
\begin{itemize}
    \item \textbf{Le Sexe :} Pour garantir le respect des tables de mortalité différenciées.
    \item \textbf{Le Taux Minimum Garanti (TMG) :} Pour éviter de diluer la valeur des garanties financières.
\end{itemize}
L'algorithme de clustering (K-Means ou HDBSCAN) est ensuite lancé indépendamment au sein de chaque strate. Cette approche garantit une « pureté » minimale des Model Points finaux.

\subsubsection*{Ingénierie des caractéristiques : Log-PM et Poids Dimensionnels}
Pour orienter les algorithmes vers les zones à fort enjeux, deux techniques de \textit{Feature Engineering} ont été testées :
\begin{itemize}
    \item \textbf{Ajout de la dimension Log-PM :} Le logarithme de la Provision Mathématique est ajouté comme variable spatiale. Cela force l'algorithme à regrouper des contrats de taille financière similaire, évitant qu'un « gros » contrat ne soit noyé dans une masse de « petits » contrats.
    \item \textbf{Pondération des dimensions :} Lors du calcul de la distance euclidienne, des poids $w_d$ sont appliqués. Par exemple, une importance de 10 est donnée au TMG contre 1 pour l'Âge, pour s'assurer que la proximité financière prime sur la proximité démographique.
\end{itemize}

\subsection{Partitionnement par inertie : \textit{k}-moyennes (K-Means) pondéré}

Contrairement aux méthodes déterministes qui segmentent l'espace des risques selon une grille préétablie, les approches par apprentissage non supervisé (Machine Learning) visent à déterminer la structure des données. L'objectif n'est plus d'imposer un regroupement, mais de laisser l'algorithme identifier les zones de forte densité du portefeuille pour y placer les groupes semblables, dans notre cas les Model Points.

Parmi les algorithmes de clustering, la méthode des K-Means a été retenue car c'est une méthode robuste et fonctionne très bien pour minimiser l'inertie intra-classe, c'est-à-dire la dispersion des contrats autour de leur Model Point représentatif.

\subsubsection{Principe de l'algorithme}
L'algorithme des K-Means cherche à partitionner un ensemble de $N$ contrats en $K$ groupes (ou clusters) distincts, de manière à minimiser la distance entre chaque contrat et le centre de son groupe (le centroïde). Dans notre contexte, ce centroïde deviendra le Model Point.

Mathématiquement, pour un ensemble de contrats représentés par des vecteurs de caractéristiques $x_i \in \mathbb{R}^d$ (où $d$ est le nombre de dimensions : Âge, Ancienneté, TMG...), l'algorithme cherche à déterminer les $K$ centroïdes $\mu_1, ..., \mu_K$ qui minimisent la fonction objectif $J$ (l'inertie) :


\begin{equation}
J = \sum_{j=1}^{K} \sum_{x_i \in C_j} w_i \left\lVert x_i - \mu_j \right\rVert^2
\end{equation}

Où :
\begin{itemize}
    \item $C_j$ est l'ensemble des contrats assignés au cluster $j$.
    \item $|| . ||$ est la distance/norme utilisée (ici norme euclidienne).
    \item $w_i$ est le poids du contrat $i$, ici cela correspond à la Provision Mathématique ($PM_i$). Ainsi, l'algorithme est forcé de minimiser l'erreur de regroupement prioritairement pour les contrats à fort enjeux financiers.
\end{itemize}

\subsubsection{Application aux données du portefeuille}
La mise en œuvre de cet algorithme sur un portefeuille d'épargne nécessite plusieurs étapes pour garantir la pertinence des regroupements : le choix des variables, la standardisation des données, et l'adaptation de l'algorithme pour intégrer la pondération par la PM.

Les variables retenues pour le calcul de la distance sont celles qui impactent directement le profil de risque et les flux futurs :
\begin{itemize}
    \item \textbf{L'Âge de l'assuré} ;
    \item \textbf{L'Ancienneté fiscale} ;
    \item \textbf{Le Taux Minimum Garanti (TMG)}.
\end{itemize}

Ces variables ayant des échelles très différentes (un âge varie de 0 à 100, un TMG de 0 à 0.04), un calcul de distance brut donnerait un poids disproportionné à l'âge. Il est donc impératif de procéder à une standardisation (centrage-réduction) des données avant le clustering :

\begin{equation}
 \tilde{x}_{i, d} = \frac{x_{i, d} - \mu_d}{\sigma_d} 
\end{equation}
Cette transformation place toutes les variables sur une échelle comparable, permettant à l'algorithme de traiter équitablement les différentes dimensions du risque.

L'algorithme utilisé est une variante pondérée des K-Means. Contrairement à une approche standard où chaque point a une importance égale, ici chaque contrat « attire » le centroïde proportionnellement à sa PM. Le processus itératif est le suivant :
\begin{enumerate}
    \item \textbf{Initialisation :} Sélection des $K$ centroïdes initiaux via la méthode \textbf{k-means++}. Contrairement à une initialisation totalement aléatoire qui risque de conduire à des optimums locaux de mauvaise qualité, cet algorithme répartit les centres initiaux de manière espacée. Le premier centroïde est choisi au hasard, puis chaque centroïde suivant est sélectionné avec une probabilité proportionnelle au carré de la distance qui le sépare du centroïde le plus proche déjà choisi ($D(x)^2$). Cela accélère considérablement la convergence.
    \item \textbf{Affectation :} Chaque contrat est assigné au centroïde le plus proche en termes de distance euclidienne pondérée.
    \item \textbf{Mise à jour :} Les nouveaux centroïdes sont recalculés comme le barycentre pondéré des contrats de leur cluster.
    \item \textbf{Convergence :} Répétition des étapes 2 et 3 jusqu'à stabilisation des centroïdes.
\end{enumerate}

\subsubsection{Constitution des Model Points finaux}
Une fois la convergence atteinte, chaque cluster $j$ est transformé en un Model Point unique.
\begin{itemize}
    \item Les variables extensives (PM, Nombre de contrats) sont sommées : $PM_{MP_j} = \sum_{i \in S_j} PM_i$.
    \item Les variables intensives (Âge, Ancienneté, TMG) sont définies par les coordonnées du centroïde final, qui correspondent naturellement à la moyenne pondérée des caractéristiques des contrats du cluster.
\end{itemize}

Cette méthode permet de définir automatiquement des Model Points situés au cœur des masses financières du portefeuille, comme illustré dans la Figure \ref{fig:kmeans_clusters}, offrant ainsi une représentation optimale de la distribution des risques.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/2_chapitres/chapitre4/kmeans_explication.png}
    \caption{Visualisation de la méthode K-Means sur un jeu de données fictif \cite{kmeans}}
    \label{fig:kmeans_clusters}
\end{figure}

Bien que l'algorithme des K-Means s'avère performant pour minimiser la variance financière, il repose sur une hypothèse géométrique forte : il tend à construire des partitions sphériques (ou convexes) et de tailles comparables. Or, la réalité d'un portefeuille d'assurance vie se caractérise souvent par des densités très hétérogènes et des formes irrégulières, liées par exemple à la commercialisation massive d'une génération spécifique de produits. Pour capter ces niches sans forcer l'homogénéité des volumes, il convient de s'affranchir de la notion de distance pure au profit d'une analyse de la densité locale.

\subsection{Partitionnement par densité : DBSCAN (introduction du Dithering et gestion du bruit par k-NN)}

Si l'algorithme des K-Means est performant pour des données réparties de manière homogène et sphérique, il montre ses limites lorsque les structures sous-jacentes du portefeuille sont complexes ou de densités variables. En effet, les données d'un portefeuille d'assurance vie présentent souvent des formes allongées ou irrégulières. Par exemple, une génération de produits vendue massivement sur une courte période crée une concentration spécifique de forme non convexe.

Pour capter ces formes complexes sans fixer \textit{a priori} le nombre de Model Points comme dans les K-Means, des méthodes basées sur la densité peuvent être mises en place : DBSCAN (\textit{Density-Based Spatial Clustering of Applications with Noise} ou \textit{Regroupement spatial basé sur la densité d'applications avec bruit}) et son extension hiérarchique HDBSCAN.

\subsubsection{Principes fondamentaux de DBSCAN}
L'algorithme DBSCAN définit un cluster comme une zone de forte densité séparée par des zones de faible densité. Il repose sur deux paramètres clés : un rayon de voisinage $\varepsilon$ (\textit{epsilon}) et un nombre minimum de points $MinPts$.

Mathématiquement, la notion de densité est formalisée par le voisinage $\varepsilon$ d'un point $x$, noté $N_\varepsilon(x)$ :
$ N_\varepsilon(x) = \{ y \in D \mid d(x,y) \le \varepsilon \} $

Un point $x$ est qualifié de \textbf{point cœur} (\textit{core point}) si son voisinage contient au moins $MinPts$ points : $|N_\varepsilon(x)| \ge MinPts$.

À partir de cette définition, les clusters sont construits par propagation de la propriété de \textit{densité-accessibilité} :
\begin{itemize}
    \item Un point $p$ est directement densité-accessible depuis $q$ si $q$ est un point cœur et $p \in N_\varepsilon(q)$.
    \item Un cluster est alors l'ensemble maximal de points connectés par cette relation de densité.
    \item Tout point n'appartenant à aucun cluster est considéré comme du \textbf{bruit} (\textit{outlier}).
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/2_chapitres/chapitre4/dbscan.png}
    \caption{Explication de l'algorithme DBSCAN (TODO : A refaire)}
    \label{fig:dbscan}
\end{figure}

\subsubsection{Adaptation aux contraintes assurantielles}
L'application directe de DBSCAN aux données brutes du portefeuille d'assurance vie utilisé se heurte à deux obstacles majeurs nécessitant des adaptations spécifiques implémentées dans le protocole : Le problème de la discrétisation et la gestion du bruit.

Les variables de gestion (Âge, Ancienneté) sont traditionnellement stockées de manière discrète. Cette structure crée une grille artificielle où plusieurs contrats se superposent exactement sur les mêmes coordonnées $(x,y)$. Cela fausse le calcul de densité locale : un point isolé sur la grille peut artificiellement paraître très dense simplement parce qu'il superpose plusieurs contrats identiques au mêmes caractéristiques.

Pour y remédier, une technique de \textbf{Dithering} (bruitage uniforme) peut être utilisée. Avant l'étape de clustering, une perturbation aléatoire $u_i$ est ajoutée aux variables temporelles de chaque contrat $x_i$ :

$ \tilde{x}_{i, d} = x_{i, d} + u_{i, d} \quad \text{avec} \quad u_{i, d} \sim \mathcal{U}([-0.5, 0.5]) $

Cette transformation permet alors de fluidifier l'espace et d'éviter une trop grande densité des contrats, sans altérer les propriétés statistiques globales du portefeuille (l'espérance de la perturbation étant nulle).

\subsubsection{Gestion du bruit et réallocation des contrats}
Contrairement à une analyse de données classique exploratoire où le bruit peut être écarté, dans un modèle épargne, la complétude des engagements est une contrainte absolue :     

$\sum PM_{MP} = \sum PM_{contrats}$.

Les contrats classés comme \og bruit \fg{} par DBSCAN (zones de faible densité ou points isolés) ne peuvent être ignorés.

Il a donc fallu implémenter une étape de post-traitement systématique : une réallocation via un algorithme des \textbf{$k$-plus proches voisins ($k$-NN)} avec $k=1$. Chaque contrat identifié comme bruit $x_{noise}$ est réaffecté au cluster validé $C_j$ le plus proche :
$ Class(x_{noise}) = Class(\underset{y \in \text{Clustered}}{\text{argmin}} \ d(x_{noise}, y)) $
Cela garantit qu'aucun contrat n'est perdu tout en rattachant les profils atypiques aux segments les plus ressemblants.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/2_chapitres/chapitre4/dbscan.png}
    \caption{Explication du reclassement du bruit (TODO : A refaire)}
    \label{fig:dbscan_bruit}
\end{figure}


\subsection{L'extension hiérarchique adaptative : HDBSCAN (Distance d'accessibilité, extraction EOM vs Leaf)}

Bien que l'algorithme DBSCAN permette de créer des clusters sur des données non sphériques comme le fait l'algorithme K-Means, il conserve une limite majeure : l'utilisation d'un seuil de densité global ($\varepsilon$) unique. Dans un portefeuille d'assurance vie, la densité des données est hétérogène. Certaines zones, correspondant aux produits récemment commercialisés, présentent une très forte concentration de contrats, tandis que d'autres, regroupant des générations anciennes ou des produits de niche, sont beaucoup plus diffuses. L'application d'un $\varepsilon$ unique conduit inévitablement à un compromis insatisfaisant : un seuil strict fragmente les zones diffuses en bruit, tandis qu'un seuil lâche fusionne des clusters distincts dans les zones denses.

Pour répondre à cette problématique, l'algorithme \textbf{HDBSCAN} (\textit{Hierarchical Density-Based Spatial Clustering of Applications with Noise}) propose une approche hiérarchique permettant de détecter des clusters de densités variables. Son fonctionnement se décompose en cinq étapes clés \cite{hdbscan} :

\subsubsection{Transformation de l'espace : la distance d'accessibilité mutuelle}
Afin de rendre l'algorithme plus robuste aux points aberrants (bruit), HDBSCAN ne travaille pas directement sur la distance euclidienne brute, mais définit une nouvelle métrique : la « distance d'accessibilité mutuelle ».

\begin{equation}
 d_{mreach}(a, b) = \max \{ \text{core}_k(a), \text{core}_k(b), d(a, b) \} 
\end{equation}

Où $\text{core}_k(x)$ est la distance du point $x$ à son $k$-ième voisin le plus proche. Cette métrique permet de pondérer la distance entre deux points par leur densité locale : si un point se trouve dans une zone de faible densité, sa distance de cœur sera élevée, augmentant ainsi sa distance mutuelle $d_{mreach}$ avec les autres points. Cette transformation a pour effet d'isoler les points aberrants en les « repoussant » hors des zones de forte concentration, ce qui stabilise la formation des clusters.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/2_chapitres/chapitre4/hdbscan1.png}
    \caption{Illustration de la distance d'accessibilité mutuelle \cite{hdbscan}}
    \label{fig:mutual_reachability}
\end{figure}

\subsubsection{Construction de l'Arbre Couvrant Minimum (MST)}
L'algorithme construit ensuite un graphe où chaque contrat est un sommet, relié aux autres par des arêtes pondérées par la distance $d_{mreach}$. Un Arbre Couvrant Minimum (Minimum Spanning Tree - MST) est généré pour connecter l'ensemble des points en minimisant le poids total des arêtes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/2_chapitres/chapitre4/hdbscan2.png}
    \caption{Construction de l'Arbre Couvrant Minimum \cite{hdbscan}}
    \label{fig:mst}
\end{figure}

\subsubsection{Construction de la hiérarchie des clusters}
En supprimant itérativement les arêtes du MST par ordre décroissant de poids, l'algorithme déconnecte progressivement le graphe. Cela crée une structure dendrogrammatique (arbre hiérarchique) représentant l'ensemble des regroupements possibles, du plus global (un seul cluster) au plus fin (chaque point est un cluster).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/2_chapitres/chapitre4/hdbscan3.png}
    \caption{Construction de la hiérarchie des clusters \cite{hdbscan}}
    \label{fig:hierarchy_clusters}
\end{figure}

\subsubsection{Condensation de l'arbre}
L'arbre hiérarchique complet étant trop complexe, il est condensé. À chaque séparation (split), la taille minimale des nouveaux groupes formés est vérifiée si les nouveaux groupes formés atteignent une taille minimale ($MinPts$). Si ce n'est pas le cas, les points sont considérés comme du bruit détaché du cluster principal. Si les deux branches sont suffisamment grandes, il est considéré qu'il y a naissance de deux vrais clusters. Cette étape simplifie drastiquement l'arbre en ne conservant que les branches significatives.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/2_chapitres/chapitre4/hdbscan4.png}
    \caption{Condensation de l'arbre hiérarchique \cite{hdbscan}}
    \label{fig:condensed_tree}
\end{figure}

\subsubsection{Extraction des clusters stables}
Contrairement aux méthodes hiérarchiques classiques qui coupent l'arbre à un niveau fixe, HDBSCAN sélectionne les clusters en maximisant une mesure de stabilité appelée \textit{Excess of Mass} (eom). La stabilité d'un cluster est définie par la somme, pour tous ses points, de la différence entre la densité à laquelle le point quitte le cluster ($\lambda_{d
eath}$) et celle où le cluster est apparu ($\lambda_{birth}$) :

\begin{equation}
 \mathcal{S}(C) = \sum_{x \in C} (\lambda_{death}(x) - \lambda_{birth}(C)) 
\end{equation}
L'algorithme remonte l'arbre condensé et sélectionne l'ensemble de clusters disjoints qui maximise cette stabilité globale.

\subsubsection{Stratégies d'extraction des clusters : EOM vs Leaf}
Deux stratégies de sélection des nœuds dans la hiérarchie ont été mises en concurrence :
\begin{itemize}
    \item \textbf{Excess of Mass (EOM) :} C'est la stratégie par défaut de HDBSCAN. Elle cherche à maximiser la stabilité globale en sélectionnant les clusters les plus persistants dans l'arbre. Elle a tendance à produire des macro-clusters, ce qui favorise un fort taux de compression mais peut lisser des micro-segments atypiques.
    \item \textbf{Leaf (Feuilles) :} Cette variante force l'algorithme à sélectionner uniquement les nœuds terminaux de l'arbre condensé (les « feuilles »). Elle garantit l'homogénéité maximale au sein de chaque Model Point en évitant toute fusion, au prix d'un nombre de MP plus important.
\end{itemize}

Cette méthodologie permet d'identifier simultanément des micro-clusters très compacts et des macro-clusters plus étendus, offrant une segmentation optimale et adaptative de la population des assurés sans paramétrage complexe d'un rayon de voisinage.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/2_chapitres/chapitre4/hdbscan6.png}
    \caption{Résultat final du clustering HDBSCAN \cite{hdbscan}}
    \label{fig:hdbscan_final}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/2_chapitres/chapitre4/dbscan_hdbscan.jpg}
    \caption{Comparaison des approches DBSCAN et HDBSCAN \cite{dbscan_hdbscan}}
    \label{fig:hdbscan_comparison}
\end{figure}

\subsubsection{Constitution finale des Model Points}
Une fois la partition optimale obtenue (via DBSCAN ou HDBSCAN) et le bruit réalloué, la construction des Model Points suit la logique de conservation des flux financiers. Pour chaque cluster $j$, le Model Point est défini par le barycentre pondéré des contrats :

$ \mu_j = \frac{\sum_{i \in C_j} PM_i \cdot x_i}{\sum_{i \in C_j} PM_i} $

Cette méthode assure que le Model Point se situe au centre de gravité financier de son groupe, minimisant ainsi le biais d'agrégation sur les projections futures.

\section{L'approche par segmentation supervisée}

Alors que les méthodes d'apprentissage non supervisé (K-Means, HDBSCAN) cherchent à regrouper les contrats selon une similarité globale de leurs caractéristiques, elles ne garantissent pas structurellement l'homogénéité d'un indicateur financier précis. Pour pallier cette limite et forcer la modélisation à se concentrer sur les enjeux de rentabilité (comme le TMG), il convient de se tourner vers des algorithmes d'apprentissage supervisé.

\subsection{Principe des Arbres de Décision (CART) appliqués au passif}

L'approche par arbres de décision, et plus précisément l'algorithme CART (\textit{Classification and Regression Trees}), introduit une logique d'apprentissage supervisé dans le processus d'agrégation. Contrairement aux K-Means qui cherchent une similarité globale, le CART vise à partitionner le portefeuille en minimisant la variance d'une variable cible d'
intérêt.

Dans le cadre de cette étude, deux variantes de ciblage ont été implémentées :
\begin{itemize}
    \item \textbf{Refined :} La cible est une combinaison linéaire de l'encours et du taux garanti ($PM \times (1 + 10 \times TMG)$), afin de forcer l'arbre à isoler les masses fi
nancières à fort enjeux de revalorisation.
    \item \textbf{Spread Focus :} La cible est le TMG pur, l'objectif étant d'obtenir des clusters d'une grande pureté financière pour limiter la dilution des garanties.
\end{itemize}

L'algorithme procède par divisions successives (splits) binaires de l'espace des caractéristiques (Âge, Ancienneté, TMG). À chaque étape, il choisit la variable et le seuil qui maximisent l'homogénéité de la cible au sein des deux groupes formés. Le processus s'arrête lorsqu'un nombre maximal de feuilles (correspondant au nombre de Model Points souhaité) est atteint.

\subsection{Définition des variables cibles (Refined vs Spread Focus)}
La puissance de l'arbre CART réside dans le choix de la métrique à optimiser :
\begin{itemize}
    \item \textbf{Variante Refined :} Une variable cible d'intérêt $Y_i$ est définie par $Y_i = PM_i \times (1 + \alpha \cdot TMG_i)$. Cette pondération force l'arbre à accorder plus d'importance aux contrats qui pèsent lourd financièrement tout en ayant un coût de garantie élevé. C'est une approche visant l'équilibre entre masse et risque.
    \item \textbf{Variante Spread Focus :} La cible est le TMG pur. L'algorithme cherche alors à créer des groupes où l'hétérogénéité des taux garantis est minimale. Cette approche est privilégiée pour les modèles ALM très sensibles au spread entre taux de marché et taux techniques.
\end{itemize}

Cette méthode présente l'avantage d'identifier automatiquement les interactions non linéaires entre les variables (par exemple, un seuil d'âge qui ne devient critique que pour une certaine ancienneté). Cependant, elle produit une segmentation par « paliers » qui peut s'avérer trop brutale pour modéliser des phénomènes continus comme la mortalité.

\section{Les approches dynamiques basées sur l'écoulement des flux}

Toutes les méthodes de partitionnement spatial abordées jusqu'ici, qu'elles soient supervisées ou non, partagent une limite conceptuelle commune : elles évaluent la similarité des profils de risque de manière purement statique, à la date d'arrêté. Pour franchir un nouveau cap de fidélité financière, une rupture méthodologique s'impose : agréger les contrats non plus sur ce qu'ils sont, mais sur ce qu'ils vont produire.

\subsection{Appariement comportemental (Cash-Flow Matching) et niveaux de fidélité (Proxy vs Real OPS)}

La méthode de \textit{Cash-Flow Matching} constitue l'approche la plus avancée de ce protocole. Elle repose sur un changement de paradigme : les contrats sont regroupés selon leur \textbf{profil de risque dynamique}.

Le principe s'articule en deux phases :
\begin{enumerate}
    \item \textbf{Phase de Projection Proxy :} Chaque contrat individuel $i$ est projeté au sein du moteur métier sous un scénario déterministe central sur un horizon de $T=20$ ans. Un vecteur de flux financiers est extrait de cette projection $\mathbf{CF}_i = [f_{i,1}, f_{i,2}, ..., f_{i,T}]$, où $f_{i,t}$ représente la somme des prestations (rachats, décès, termes) et de la marge financière à l'année $t$.
    \item \textbf{Phase de Clustering :} L'algorithme des K-Means est ensuite appliqué non pas sur les données de gestion, mais sur ces vecteurs de flux $\mathbf{CF}_i$. La distan
ce entre deux contrats est alors définie par leur proximité comportementale dans le temps :
    
\begin{equation}
 d(i, j) = \sqrt{\sum_{t=1}^{T} w_t (f_{i,t} - f_{j,t})^2} 
\end{equation}
\end{enumerate}

\subsubsection{Niveaux de fidélité du moteur proxy}
Cette approche a été déclinée en deux versions selon la complexité du moteur de projection utilisé pour générer les vecteurs $\mathbf{CF}_i$ :
\begin{itemize}
    \item \textbf{Mode Proxy (Simple) :} Utilise un modèle simplifié, souvent linéaire ou déterministe de premier ordre, pour projeter les flux. C'est une méthode rapide mais qui peut négliger les effets de second ordre comme l'épuisement de la PPE ou les options de rachat dynamique.
    \item \textbf{Mode Real OPS (Moteur Métier) :} Utilise le véritable moteur de calcul Polars développé pour ce mémoire. Bien que plus coûteux en temps de préparation (env. 30s), il garantit que le clustering s'opère sur la réalité économique exacte du contrat, capturant l'intégralité des non-linéarités (participation aux bénéfices, rachats structurels, fiscalité).
\end{itemize}

Cette approche, désignée sous le terme \textbf{Real OPS} dans cette étude, permet de capturer intrinsèquement toutes les non-linéarités du modèle ALM. Deux contrats présentant des caractéristiques très différentes (par exemple, un assuré jeune sur un contrat ancien et un assuré âgé sur un contrat récent) peuvent être regroupés s'ils génèrent des flux de sortie statistiquement équivalents.

L'avantage majeur est la réduction drastique du « biais d'agrégation » puisque le critère de regroupement est précisément l'indicateur calculé \textit{in fine} (le Best Estimate). L'inconvénient réside dans le coût computationnel de la pré-projection initiale, qui doit être compensé par le gain de temps lors des lancements ALM ultérieurs.

\subsection{Calibration de lois a posteriori (Méthode "Biais Zéro" et clés financières)}

L'approche par calibration \textit{a posteriori}, dénommée \og Biais Zéro \fg{} dans cette étude, marque une rupture méthodologique avec les techniques de clustering spatial. Là où les méthodes classiques tentent d'ajuster des contrats à des lois de mortalité existantes, cette approche propose d'adapter les lois de sortie pour qu'elles correspondent parfaitement à un regroupement arbitraire de contrats.

\subsubsection{Mécanique des décréments et bases de calibration}

Pour atteindre une erreur nulle sur le Best Estimate, la calibration doit respecter scrupuleusement l'ordonnancement des flux au sein du moteur de projection Polars. Le moteur applique la séquence suivante sur une période $[t, t+1]$ :
\begin{enumerate}
    \item \textbf{Revalorisation technique} : Application du TMG au prorata temporis sur la PM de début d'année.
    \item \textbf{Calcul des rachats} : Les rachats s'appliquent sur la PM revalorisée à mi-période.
    \item \textbf{Calcul des décès} : Les décès s'appliquent sur la PM nette des rachats.
\end{enumerate}

Ainsi, les lois induites $q_{k,t}$ ne sont pas calculées sur la PM de début d'année, mais sur les \textbf{bases d'application exactes} (mi-période). Pour un groupe de contrats $k$, les probabilités de rachat ($q_{k,t}^{rt}$) et de décès ($q_{k,t}^{dc}$) sont définies par :

\begin{equation}
    q_{k,t}^{rt} = \frac{\sum_{i \in C_k} PR_{i,t}}{\sum_{i \in C_k} BR_{i,t}} \quad ; \quad q_{k,t}^{dc} = \frac{\sum_{i \in C_k} PD_{i,t}}{\sum_{i \in C_k} BD_{i,t}}
\end{equation}

Où $PR_{i,t}$ et $PD_{i,t}$ représentent respectivement les prestations pour rachats et pour décès du contrat $i$ à l'instant $t$. Les termes $BR_{i,t}$ et $BD_{i,t}$ désignent les bases d'application exactes associées.

Où la base de rachat intègre les intérêts techniques acquis à mi-période, et la base de décès est nette des rachats déjà effectués. Cette précision mathématique permet de capturer l'intégralité de la cinétique des flux, rendant le Model Point agnostique de la démographie réelle ($Age, Sexe$).

\subsubsection{Définition des clés d'agrégation financières}

Le portefeuille est partitionné selon un triplet de variables financières qui régissent la mécanique de revalorisation :
\begin{itemize}
    \item \textbf{Le TMG net} : Le taux minimum garanti servi à l'assuré.
    \item \textbf{Le TFGSE} : Le taux de frais de gestion sur encours prélevé par l'assureur.
    \item \textbf{Le TAF} : Le taux d'affectation des produits financiers.
\end{itemize}

Cette segmentation garantit que le \textbf{Model Point Financier (MPF)} possède un seuil de rentabilité brut identique à celui des contrats individuels :
\begin{equation}
    TMG_{brut} = \frac{TMG_{net} + TFGSE}{TAF}
\end{equation}

L'agrégation sur ces clés, combinée aux lois induites, permet d'atteindre le \og Biais Zéro \fg{} opérationnel, l'erreur résiduelle ne provenant plus que des arrondis numériques (estimée à $0,006\%$ dans nos tests).

\section{Conclusion}

Ce chapitre a permis d'exposer le socle théorique des différentes méthodes d'agrégation envisagées, de la rigidité des approches déterministes classiques à la flexibilité des algorithmes d'apprentissage automatique spatial, jusqu'à la complexité des modèles d'appariement de flux financiers. Chaque méthode présente des compromis distincts entre la précision de la modélisation, le respect des garanties financières et le coût computationnel. 

Cependant, l'évaluation purement théorique de ces approches demeure insuffisante pour statuer sur leur applicabilité. Il est désormais impératif de confronter ces méthodes à la réalité d'un moteur de projection stochastique. Le chapitre suivant s'attachera ainsi à définir un protocole de test standardisé et à mener une analyse comparative rigoureuse, afin d'identifier la méthodologie optimale pour l'évaluation des passifs sous le référentiel Solvabilité 2.